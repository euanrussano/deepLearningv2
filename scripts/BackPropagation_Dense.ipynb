{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropagation Dense.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpIs0RWOYS9Y",
        "outputId": "e0c0225c-09b8-4efc-fc63-e890da229e8e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 1, 2, 1, 2\n",
        "\n",
        "# Create random input and output data\n",
        "x = np.round(np.random.randn(N, D_in),2)\n",
        "print('x = ', x)\n",
        "y = np.round(np.random.randn(N, D_out),2)\n",
        "print('y = ', y)\n",
        "\n",
        "# Randomly initialize weights\n",
        "w1 = np.round(np.random.randn(D_in, H),2)\n",
        "w2 = np.round(np.random.randn(H, D_out),2)\n",
        "print('w1 = ',w1)\n",
        "print('w2 = ',w2)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(1):\n",
        "    # Forward pass: compute predicted y\n",
        "    a1 = x.dot(w1)\n",
        "    print('h = ',h)\n",
        "    h_relu = np.maximum(a1, 0)\n",
        "    print('h_relu = ',h_relu)\n",
        "    a2 = h_relu.dot(w2)\n",
        "    print('a2 = ',a2)\n",
        "    y_pred = np.maximum(a2, 0)\n",
        "    print('y_pred = ',y_pred)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = np.square(y_pred - y).sum()\n",
        "    print(t, loss)\n",
        "\n",
        "    # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    grad_y_pred = 2.0 * (y_pred - y)\n",
        "    print('grad_y_pred = ', grad_y_pred)\n",
        "    grad_a2 = grad_y_pred.copy()\n",
        "    grad_a2[a2 < 0] = 0\n",
        "    print('grad_a2 = ', grad_a2)\n",
        "    grad_w2 = h_relu.T.dot(grad_a2)\n",
        "    print('grad_w2 = ', grad_w2)\n",
        "    grad_h_relu = grad_a2.dot(w2.T)\n",
        "    print('grad_h_relu = ', grad_h_relu)\n",
        "    grad_a1 = grad_h_relu.copy()\n",
        "    \n",
        "    grad_a1[a1 < 0] = 0\n",
        "    print('grad_a1 = ', grad_a1)\n",
        "    grad_w1 = x.T.dot(grad_a1)\n",
        "    print('grad_w1 = ', grad_w1)\n",
        "\n",
        "    # Update weights\n",
        "    w1 -= learning_rate * grad_w1\n",
        "    w2 -= learning_rate * grad_w2\n",
        "\n",
        "    print('w1 = ',w1)\n",
        "    print('w2 = ',w2)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x =  [[ 1.62 -0.61]]\n",
            "y =  [[-0.53 -1.07]]\n",
            "w1 =  [[ 0.87]\n",
            " [-2.3 ]]\n",
            "w2 =  [[ 1.74 -0.76]]\n",
            "h =  [[2.8124]]\n",
            "h_relu =  [[2.8124]]\n",
            "a2 =  [[ 4.893576 -2.137424]]\n",
            "y_pred =  [[4.893576 0.      ]]\n",
            "0 30.560076627775995\n",
            "grad_y_pred =  [[10.847152  2.14    ]]\n",
            "grad_a2 =  [[10.847152  0.      ]]\n",
            "grad_w2 =  [[30.50653028  0.        ]]\n",
            "grad_h_relu =  [[18.87404448]]\n",
            "grad_a1 =  [[18.87404448]]\n",
            "grad_w1 =  [[ 30.57595206]\n",
            " [-11.51316713]]\n",
            "w1 =  [[ 0.86996942]\n",
            " [-2.29998849]]\n",
            "w2 =  [[ 1.73996949 -0.76      ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ8l9IYoc5l_",
        "outputId": "06058d2c-38be-4842-c538-46beed88bc18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import torch\n",
        "from torch.nn import Sigmoid\n",
        "sig = Sigmoid()\n",
        "\n",
        "np.random.seed(1)\n",
        "torch.manual_seed(1)\n",
        "\n",
        "dtype = torch.float\n",
        "device = torch.device(\"cpu\")\n",
        "\n",
        "# Create random Tensors to hold input and outputs.\n",
        "# Setting requires_grad=False indicates that we do not need to compute gradients\n",
        "# with respect to these Tensors during the backward pass.\n",
        "x = torch.tensor([[ 1.62, -0.61]])\n",
        "y = torch.tensor([[-0.53, -1.07]])\n",
        "\n",
        "# Create random Tensors for weights.\n",
        "# Setting requires_grad=True indicates that we want to compute gradients with\n",
        "# respect to these Tensors during the backward pass.\n",
        "w1 = torch.tensor([[ 0.87],[-2.3 ]], requires_grad = True)\n",
        "w2 = torch.tensor([[ 1.74, -0.76]], requires_grad=True)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "for t in range(1):\n",
        "    # Forward pass: compute predicted y using operations on Tensors; these\n",
        "    # are exactly the same operations we used to compute the forward pass using\n",
        "    # Tensors, but we do not need to keep references to intermediate values since\n",
        "    # we are not implementing the backward pass by hand.\n",
        "    y_pred = x.mm(w1).clamp(min=0).mm(w2).clamp(min=0)\n",
        "\n",
        "    # Compute and print loss using operations on Tensors.\n",
        "    # Now loss is a Tensor of shape (1,)\n",
        "    # loss.item() gets the scalar value held in the loss.\n",
        "    loss = (y_pred - y).pow(2).sum()\n",
        "    if t % 100 == 99:\n",
        "        print(t, loss.item())\n",
        "\n",
        "    # Use autograd to compute the backward pass. This call will compute the\n",
        "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
        "    # After this call w1.grad and w2.grad will be Tensors holding the gradient\n",
        "    # of the loss with respect to w1 and w2 respectively.\n",
        "    loss.backward()\n",
        "\n",
        "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
        "    # because weights have requires_grad=True, but we don't need to track this\n",
        "    # in autograd.\n",
        "    # An alternative way is to operate on weight.data and weight.grad.data.\n",
        "    # Recall that tensor.data gives a tensor that shares the storage with\n",
        "    # tensor, but doesn't track history.\n",
        "    # You can also use torch.optim.SGD to achieve this.\n",
        "    with torch.no_grad():\n",
        "        w1 -= learning_rate * w1.grad\n",
        "        w2 -= learning_rate * w2.grad\n",
        "\n",
        "        print(w2.grad)\n",
        "        print(w1.grad)\n",
        "        \n",
        "        # Manually zero the gradients after updating weights\n",
        "        w1.grad.zero_()\n",
        "        w2.grad.zero_()"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[30.5065,  0.0000]])\n",
            "tensor([[ 30.5760],\n",
            "        [-11.5132]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjQFw1GFuxWs",
        "outputId": "ca9bf595-ac79-4fb0-808d-c736a1c8fe68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.0100]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hC8393RgwYR1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}